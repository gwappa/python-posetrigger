= Pose-Trigger
:toc:

Pose-Trigger is a python application for real-time, closed-loop application
of TTL trigger generation based on the pose of the subject.

image::resources/Screenshot.png[A screenshot of a working Pose-Trigger app (in a development version)]

== Introduction

=== What can Pose-Trigger do?

Pose-Trigger is designed to work on a linux computer equipped with a high-speed video camera.
The current version of the software features:

. Acquisition of *high-speed videos* (up to 100-200 fps without on-line pose estimation).
.. On-line exposure/gain adjustment.
.. Adjustment of acquisition intervals.
. *On-line estimation of body-part positions* using http://www.mousemotorlab.org/deeplabcut[DeepLabCut].
.. On-line evaluation of *arbitrary posture conditions* based on the estimated body-part positions.
.. *Fast output-trigger generation* (<1 ms) using the https://doi.org/10.5281/zenodo.3843623[FastEventServer] program.
. *Brightness/contrast adjustment* for on-line display.
. *Storage of frames* into the NumPy-style zip archive.

== Installation

=== Requirements

==== Minimum requirements

At the very basics (i.e. acquisition), you need the followings:

. *A linux computer* (tested on https://releases.ubuntu.com/18.04.5/[Ubuntu 18.04 LTS])
. An installation of **Python, version >=3.4**. We recommend installing the following libraries using e.g. https://www.anaconda.com/[Anaconda]:
.. NumPy
.. Matplotlib
.. python-opencv
.. PyQt (required for pyqtgraph)
.. http://pyqtgraph.org/[pyqtgraph] (through `pip`, instead of through `conda`)
. A Video4Linux2-compliant *(high-speed) 16-bit monochrome video camera* (recommended to use devices from https://www.theimagingsource.com/[ImagingSource] since the program is tuned up for them)

==== Requirements for on-line position estimation

In addition, the on-line position-estimation feature requires the followings in your environment:

. An installation of http://www.mousemotorlab.org/deeplabcut[*DeepLabCut*] (any versions after 1.11 should work).
. For a faster working of DeepLabCut, *NVIDIA graphics board with a large amount of RAM* would be needed. For example, running DeepLabCut on ResNet-50 requires ~10.6 GB of RAM, so we use https://www.nvidia.com/en-eu/geforce/graphics-cards/rtx-2080-ti/[GeForce RTX 2080 Ti] that has 11 GB on-board RAM.

==== Requirements for fast-trigger generation

Finally, the fast-trigger feature requires the followings:

. The https://doi.org/10.5281/zenodo.3843623[*FastEventServer*] *server program*. If you use a 64-bit linux computer (it is most likely these days), you can use the `FastEventServer_linux_64bit` pre-built program bundled with the repository, and do not have to build it yourself.
. An https://store.arduino.cc/arduino-uno-rev3[*Arduino UNO*] or its clone, flashed using the https://doi.org/10.5281/zenodo.3515998[arduino-fasteventtrigger] program.

[IMPORTANT]
=========
*IMPORTANT NOTE*: `arduino-fasteventtrigger`, in reality, will *only make use of the serial-to-USB conversion tip on the UNO (i.e. https://www.microchip.com/wwwproducts/en/ATmega16U2[ATmega16U2])*. This means:

- Make sure that your UNO clone has the ATmega16U2 as its converter chip.
- Other USB-based boards that uses the ATmega16U2 chip _may_ work (although not recommended or supported).

=========

=== Install procedures

Install all the python packages in your DeepLabCut environment, in case it applies.

. (Optional) set up a DeepLabCut environment.
. Install the libraries specified in "minimum requirements".
. Install `timedcapture`: this is the library for video acquisition. Follow the installation procedure in the repository.
. Install the `pose-trigger` module:
.. Clone this repository.
.. Open this repository in `Terminal`.
.. Run `pip install .` on `Terminal`.

[TIP]
Upon the public release of Pose-Trigger in the future, both `timedcapture` and `pose-trigger` packages will be made available in PyPI. One will be able to install these packages through `pip`.

== User guide

=== Launching Pose-Trigger

. Open `Terminal`.
. Run `pose-trigger` on `Terminal`.

[TIP]
When being run without a parameter, Pose-Trigger will use the device on `/dev/video0` by default. In case you want to use e.g. `/dev/video1`, specify the device as the parameter, i.e. run `pose-trigger /dev/video1`.

=== Capturing videos

==== Capture modes

There are two modes of running for Pose-Trigger:

. *FOCUS mode*: capturing video frames without storing them
. *ACQUIRE mode*: captures video frames _and_ stores acquired data

You can start/stop either of the capturing modes by clicking on the button at the bottom of the main window.

[IMPORTANT]
====

Currently, the following parameters are "hard-coded" and used as default:

- Image format: 640x480 pixels, 16-bit grayscale
- Timing generation: a busy-wait algorithm
- Storage format: the NumPy zip-file format (.npz)

====

==== Format of the saved files

The data are saved in the NumPy zip-file format (i.e. ".npz" file). Each file includes the following entries:

(TODO)


=== Panel guide: an overview

image::resources/Layout_Overview.png[Overview of the main window]

The Pose-Trigger main window can be divided into three groups:

. The *Capture* buttons (yellow) is for starting/stopping acquisition.
. The *Preview* panel (green) is an on-line preview of the acquired video frames. If body position-estimation is activated, estimated positions will be shown as colored circles, too.
. In the *Settings* panel (blue), you can configure how acquisition is performed.

=== Panel guide: individual settings

==== "Camera" panel

image::resources/Panels_camera.png[Capture parameter settings]

Here, you can set the exposure and the gain of each video frame acquisition.

[NOTE]
For the time being, the image format is restricted to 16-bit grayscale, with the 640x480 frame size (otherwise there will be an unexpected behavior).

==== "Preprocessing" panel

image::resources/Panels_preprocessing.png[Preprocessing settings]

This controls the brightness/contrast settings for "live" video frames. It controls signal conditioning parameters for:

- Video-frame preview
- Body-part estimation (the images being fed to DeepLabCut)

On the other hand, *the raw, unconditioned images are used* for data storage.

==== "Acquisition" panel

image::resources/Panels_acquisition.png[Acquisition timing control]

Here you can set the (targeted) acquisition intervals. For example, if you want to have Pose-Trigger running at 50 Hz, set this interval to 20 ms.

[NOTE]
For the time being, you can only choose to use the busy-wait timing generation.

==== "DeepLabCut evaluation" panel

image::resources/Panels_evaluation.png[Evaluation mode control]

Here, you can configure how DeepLabCut should work in real-time.

===== Project selection

By using the "Select" button, you can select your DeepLabCut project of choice. Conversely, by clicking on the "Clear" button, you can un-set the project.

When a project is selected, the panel shows the body-part labels being registered in the project.

As long as a project is selected here, body-part position estimation occurs during video-capture processes. Estimated positions will also be stored in the data file in the case of the `ACQUIRE` mode.

===== Pose evaluation

You can enable pose evaluation by ticking the "Enable evaluation" button. Evaluation occurs using *the boolean expression entered in the "Expression" field*. The "expression" can be any Python one-line expression, but it has to be evaluated to be a boolean.

When specifying the boolean expression, you can use a *placeholder-based reference* to body part positions. For example, by entering `{Tip1.x}`, you can use the X coordinate of `Tip1` as a parameter. Other than the `x` property, you can also use the `y` and `p` properties of a body part to refer to the Y coordinate and the probability

In computation of the expression, some major libraries can be used: use `math` for representing the `math` standard library, and use `np` to refer to the `numpy` library. For example, the expression below calculates the Euclidean distance between two body parts, `Tip1` and `Tip2`:

[source]
--
math.sqrt( ({Tip1.x} - {Tip2.x})**2 + ({Tip1.y} - {Tip2.y})**2 )
--

In addition, to enable testing of the output latency at the trigger-generation step, the custom placeholder, `{EVERY10}` is there. By using the following expression, you can toggle trigger output on and off every 10 frames:

[source]
--
{EVERY10}.get()
--


==== "Trigger generation" panel

image::resources/Panels_triggering.png[Trigger mode control]

Here, you can test and control trigger generation.

#### Transferring evaluation results to FastEventServer

By ticking "Enable trigger output", it starts sending the result of evaluation (true/false value) to FastEventServer.

#### Manually toggling the trigger

When trigger-output based on evaluation results is disabled, you can manually toggle the trigger output on and off, using the "Toggle manually" button.

[IMPORTANT]
For the time being, the "trigger UDP port" cannot be specified; if Pose-Trigger fails to connect to FastEventServer on port 11666 at the beginning of its launching, it just disables the trigger-output functionality.

==== "Storage" panel

image::resources/Panels_storage.png[Storage control]

Here, you can control how acquired data are stored.

*File names are automatically generated* using the text entered in the "File-name format" field. You can use the following *format directives*. These fields are passed on straight to the `datetime.strftime` method (refer to https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior[the python datetime module documentation] on the specific format directives).

[IMPORTANT]
Be cautious that Pose-Trigger *automatically overwrites* the file! Try to include (at least) the minutes/seconds directive into the file-name format, so that you do not unexpectedly delete your previous videos. 

== How Pose-Trigger works: concepts

(TODO)
